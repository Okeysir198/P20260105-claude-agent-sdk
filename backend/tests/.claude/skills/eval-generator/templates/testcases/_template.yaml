# =============================================================================
# TEST CASE TEMPLATE
# =============================================================================
# Use this template when generating test case files for sub-agents.
# Replace placeholders in {curly_braces} with actual values.
#
# File naming convention:
#   agent01_{sub_agent_id}.yaml  - Unit tests for a sub-agent
#   agent00_e2e_flow.yaml        - End-to-end workflow tests
#   e2e_critical.yaml            - Minimal critical path tests
#
# =============================================================================

# Metadata - identifies which agent/sub-agent these tests are for
agent_id: {agent_id}
sub_agent_id: {sub_agent_id}
description: "Tests for {sub_agent_id} - {purpose_description}"

# =============================================================================
# DEFAULT TEST DATA
# =============================================================================
# Applied to ALL tests in this file unless overridden per test.
# Use realistic but fake data that matches your agent's UserData schema.
#
default_test_data:
  # Required fields (adapt to your schema)
  full_name: "Test User"
  user_id: "U00001"
  # Optional: Add domain-specific fields
  # email: "test@example.com"
  # account_status: "active"
  # outstanding_amount: 1000.00

# =============================================================================
# TEST CASES
# =============================================================================
test_cases:
  # ---------------------------------------------------------------------------
  # UNIT TEST - Single Turn (Happy Path)
  # ---------------------------------------------------------------------------
  - name: "{PREFIX}-001: {scenario_description}"
    test_type: single_turn
    tags: [unit, {sub_agent_id}, happy-path]
    turns:
      - user_input: "{what_the_user_says}"
        assertions:
          # Verify expected tool was called
          - type: contains_function_call
            value: "{expected_tool_name}"
          # Verify response quality (optional)
          - type: llm_rubric
            value: "Agent should {expected_behavior}"

  # ---------------------------------------------------------------------------
  # UNIT TEST - Single Turn (Alternative Scenario)
  # ---------------------------------------------------------------------------
  - name: "{PREFIX}-002: {alternative_scenario}"
    test_type: single_turn
    tags: [unit, {sub_agent_id}]
    turns:
      - user_input: "{alternative_user_input}"
        assertions:
          - type: contains_function_call
            value: "{expected_tool}"

  # ---------------------------------------------------------------------------
  # UNIT TEST - Single Turn (Edge Case)
  # ---------------------------------------------------------------------------
  - name: "{PREFIX}-003: {edge_case_description}"
    test_type: single_turn
    tags: [unit, {sub_agent_id}, edge-case]
    # Override default data for this specific test
    test_data:
      # Add field overrides here
      # status: "special_case"
    turns:
      - user_input: "{edge_case_input}"
        assertions:
          - type: llm_rubric
            value: "Agent should handle {edge_case} appropriately"

  # ---------------------------------------------------------------------------
  # MULTI-TURN TEST (Integration)
  # ---------------------------------------------------------------------------
  - name: "{PREFIX}-010: {workflow_description}"
    test_type: multi_turn
    tags: [integration, {sub_agent_id}]
    turns:
      # Turn 1
      - user_input: "{first_user_message}"
        assertions:
          - type: contains_function_call
            value: "{first_expected_tool}"

      # Turn 2
      - user_input: "{second_user_message}"
        assertions:
          - type: llm_rubric
            value: "Agent should {second_expected_behavior}"

# =============================================================================
# ASSERTION TYPE REFERENCE
# =============================================================================
#
# contains_function_call - Verify a tool/function was invoked
#   - type: contains_function_call
#     value: "tool_name"
#     arguments:           # Optional: verify specific arguments
#       field_name: "value"
#
# llm_rubric - LLM evaluates response quality (semantic check)
#   - type: llm_rubric
#     value: "Agent should be empathetic and offer alternatives"
#   OR
#   - type: llm_rubric
#     rubric: "Response should acknowledge customer frustration"
#
# contains - Check if response contains substring (case-insensitive)
#   - type: contains
#     value: "payment"
#
# not_contains - Ensure response does NOT contain forbidden content
#   - type: not_contains
#     value: "goodbye"
#
# contains_any - Check if response contains at least one keyword
#   - type: contains_any
#     value: ["payment", "settle", "arrange"]
#
# =============================================================================
# NAMING CONVENTIONS
# =============================================================================
#
# Test name format: "{PREFIX}-{NUMBER}: {Description}"
#   - PREFIX: 3-letter sub-agent code (e.g., INT, VER, NEG)
#   - NUMBER: 3-digit test number
#   - Description: Human-readable scenario
#
# Examples:
#   - "INT-001: Person confirms identity"
#   - "VER-003: ID number mismatch"
#   - "E2E-001: Happy path - Full workflow"
#
# =============================================================================
# TAGS REFERENCE
# =============================================================================
#
# Granularity:
#   - unit: Single turn tests
#   - integration: Multi-turn within sub-agent
#   - e2e: End-to-end across multiple agents
#
# Priority:
#   - critical: Must-pass tests
#   - happy-path: Expected success flow
#   - edge-case: Boundary conditions
#
# Agent-specific:
#   - {sub_agent_id}: Tag with the sub-agent name
#
# =============================================================================
